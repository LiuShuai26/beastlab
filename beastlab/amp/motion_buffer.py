"""
Motion clip buffer for AMP training.

Loads keyframe JSON files and provides random sampling of (s_t, s_{t+1})
transition pairs for discriminator training.

Per-frame AMP state features (matching Brain.cpp observation layout):
    [pelvis_y, joint_sin_cos(24), body_pos_pelvis_frame(10)] = 35 dims

- Joints are stored as sin/cos pairs (not raw angles) to match the raw
  observation format and avoid lossy atan2 roundtrips.
- Body positions are rotated into the pelvis local frame using the same
  transform as Brain.cpp:  dx*cos(-a) - dy*sin(-a), dx*sin(-a) + dy*cos(-a).
- pelvis_x is excluded (horizontal translation invariance).
"""

import json
import math

import numpy as np
import torch


class AMPMotionBuffer:
    """Buffer of reference motion transition pairs for AMP discriminator.

    Pre-computes per-frame AMP states from keyframe data, applying the
    same coordinate transforms as Brain.cpp, then builds consecutive
    (s_t, s_{t+1}) transition pairs.  For cyclic motions the last frame
    wraps to the first so no transitions are lost.

    Args:
        keyframe_files: Single path or list of paths to keyframe JSON files.
        joint_order:    Joint names in the order they appear in the observation.
                        Must match Brain.cpp JOINT_TAGS.
        body_order:     Key body names in the order they appear in the observation.
                        Must match HumanoidConfig.h KEY_BODY_TAGS.
        device:         Torch device for pre-computed tensors.

    Attributes:
        obs_dim:        Dimensionality of a single-frame AMP state (35).
        transition_dim: Dimensionality of a transition pair (70).
    """

    def __init__(
        self,
        keyframe_files,
        joint_order,
        body_order,
        device="cpu",
    ):
        if isinstance(keyframe_files, str):
            keyframe_files = [keyframe_files]

        self.device = device
        self.joint_order = list(joint_order)
        self.body_order = list(body_order)
        self.num_joints = len(self.joint_order)
        self.num_bodies = len(self.body_order)
        # 1 (pelvis_y) + num_joints*2 (sin/cos) + num_bodies*2 (x/y)
        self.obs_dim = 1 + self.num_joints * 2 + self.num_bodies * 2
        self.transition_dim = self.obs_dim * 2

        clips = []  # list of (F, obs_dim) arrays, one per file

        for kf_file in keyframe_files:
            with open(kf_file) as f:
                data = json.load(f)
            keyframes = data["keyframes"]

            clip = np.zeros((len(keyframes), self.obs_dim), dtype=np.float32)
            for i, kf in enumerate(keyframes):
                clip[i] = self._keyframe_to_amp(kf)
            clips.append(clip)

        # Build cyclic transition pairs per clip, then concatenate
        all_transitions = []
        total_frames = 0
        for clip in clips:
            total_frames += len(clip)
            s_tp1 = np.roll(clip, -1, axis=0)  # cyclic: last wraps to first
            transitions = np.concatenate([clip, s_tp1], axis=-1)
            all_transitions.append(transitions)

        all_transitions = np.concatenate(all_transitions, axis=0)
        self.num_transitions = len(all_transitions)
        self.amp_transitions = torch.tensor(all_transitions, device=device)

        # Keep single-frame obs for debugging
        all_frames = np.concatenate(clips, axis=0)
        self.amp_obs = torch.tensor(all_frames, device=device)
        self.num_frames = total_frames

        # Normalization stats computed from reference motion data
        self.obs_mean = self.amp_obs.mean(dim=0)
        raw_std = self.amp_obs.std(dim=0)
        self.obs_std = raw_std.clamp(min=0.1)

        # Build feature names for diagnostics
        feature_names = ["pelvis_y"]
        for jname in self.joint_order:
            feature_names.append(f"{jname}_sin")
            feature_names.append(f"{jname}_cos")
        for bname in self.body_order:
            feature_names.append(f"{bname}_x")
            feature_names.append(f"{bname}_y")

        print(
            f"AMPMotionBuffer: {total_frames} frames, "
            f"{self.num_transitions} transitions, "
            f"obs_dim={self.obs_dim}, transition_dim={self.transition_dim}"
        )
        print(f"  Joints ({self.num_joints}): {self.joint_order}")
        print(f"  Bodies ({self.num_bodies}): {self.body_order}")
        for i, (name, std) in enumerate(zip(feature_names, raw_std)):
            if std < 0.05:
                print(f"  WARNING: dim {i} ({name}) std={std:.6f}")

    def _keyframe_to_amp(self, kf):
        """Convert one keyframe dict to a 35-dim AMP state vector.

        Matches the Brain.cpp observation format:
          [pelvis_y, sin(j0), cos(j0), ..., body0_lx, body0_ly, ...]
        """
        feat = []

        # 1. pelvis_y (height)
        feat.append(kf.get("pelvis_y", 0.0))

        # 2. Joint angles â†’ sin/cos pairs (same order as Brain.cpp)
        for jname in self.joint_order:
            angle = kf.get(jname, 0.0)
            feat.append(math.sin(angle))
            feat.append(math.cos(angle))

        # 3. Body positions rotated into pelvis local frame
        #    Brain.cpp:  cosA = cos(-pelvis_angle);  sinA = sin(-pelvis_angle);
        #                local_x = dx * cosA - dy * sinA
        #                local_y = dx * sinA + dy * cosA
        #    Keyframe JSON body_x/y are world-frame offsets (world - pelvis).
        pelvis_angle = kf.get("pelvis_angle", 0.0)
        cos_a = math.cos(-pelvis_angle)
        sin_a = math.sin(-pelvis_angle)

        for bname in self.body_order:
            dx = kf.get(f"{bname}_x", 0.0)
            dy = kf.get(f"{bname}_y", 0.0)
            local_x = dx * cos_a - dy * sin_a
            local_y = dx * sin_a + dy * cos_a
            feat.append(local_x)
            feat.append(local_y)

        return np.array(feat, dtype=np.float32)

    def normalize(self, amp_obs):
        """Normalize AMP observations using reference motion statistics.

        Applied to both real and fake inputs before the discriminator so
        scale differences don't provide a trivial classification signal.
        For transition pairs, normalizes each half independently.
        Output is clamped to [-5, 5] to prevent extreme values when
        policy states are far from the reference distribution.

        Args:
            amp_obs: (B, obs_dim) single frames or (B, transition_dim) pairs.
        """
        if amp_obs.shape[-1] == self.transition_dim:
            s_t = (amp_obs[..., :self.obs_dim] - self.obs_mean) / self.obs_std
            s_tp1 = (amp_obs[..., self.obs_dim:] - self.obs_mean) / self.obs_std
            return torch.clamp(torch.cat([s_t, s_tp1], dim=-1), -5.0, 5.0)
        return torch.clamp((amp_obs - self.obs_mean) / self.obs_std, -5.0, 5.0)

    def sample(self, batch_size):
        """Sample random transition pairs.

        Returns:
            Tensor of shape (batch_size, transition_dim) on self.device.
        """
        indices = torch.randint(0, self.num_transitions, (batch_size,))
        return self.amp_transitions[indices]

    def sample_frames(self, batch_size):
        """Sample random single-frame AMP observations (for debugging).

        Returns:
            Tensor of shape (batch_size, obs_dim) on self.device.
        """
        indices = torch.randint(0, self.num_frames, (batch_size,))
        return self.amp_obs[indices]
